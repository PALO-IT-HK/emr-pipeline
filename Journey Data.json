{"paragraphs":[{"text":"%md\n\n# JourneyData\n* Time Aggregated Journey Data (Parquet): `/processed/journey`\nStation Data `id`, `lat`, `lng`, `district`, `location` / `time` (10-minute-group) / count(`start`) as `start_count` / count(`end`) as `end_count`\n\n* BikeStation Aggregated Journey Data (Parquet): `/processed/bikestation`\n`Date` / `Time` / `Direction` \\(\"departure\"|\"arrival\"\\) / `SrcOrDescId` / count()","user":"anonymous","dateUpdated":"2018-04-04T03:39:40+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{"YEAR":"","MONTH":"","DAY":""},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>JourneyData</h1>\n<ul>\n<li><p>Time Aggregated Journey Data (Parquet): <code>/processed/journey</code>\n<br  />Station Data <code>id</code>, <code>lat</code>, <code>lng</code>, <code>district</code>, <code>location</code> / <code>time</code> (10-minute-group) / count(<code>start</code>) as <code>start_count</code> / count(<code>end</code>) as <code>end_count</code></p>\n</li>\n<li><p>BikeStation Aggregated Journey Data (Parquet): <code>/processed/bikestation</code>\n<br  /><code>Date</code> / <code>Time</code> / <code>Direction</code> (&ldquo;departure&rdquo;|&ldquo;arrival&rdquo;) / <code>SrcOrDescId</code> / count()</p>\n</li>\n</ul>\n"}]},"apps":[],"jobName":"paragraph_1522744779044_244080242","id":"20180403-083939_1463318923","dateCreated":"2018-04-03T08:39:39+0000","dateStarted":"2018-04-04T03:39:40+0000","dateFinished":"2018-04-04T03:39:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1526"},{"text":"%pyspark\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession\\\n    .builder\\\n    .appName(\"JourneyData-ETL\")\\\n    .getOrCreate()\n    \n# Dev\ndebug = True\n\n# S3 Bucket for raw data\ns3_bucket = \"beda-emr-testdata\"\n\n# Other \"static\" data\nbikepoints_file = \"/bikepoints/latlong/all-bikepoints-latlong.min.json\"\nlondon_hourly_weather_file = \"/weather-london.csv\"\n    \n# Bike journey history\nraw_path = \"/raw\"\nprocessed_path = \"/processed\"\nfiles = \"/*/*.csv\"\n","user":"anonymous","dateUpdated":"2018-04-06T09:13:30+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1522691924553_1628501184","id":"20180402-175844_2028336576","dateCreated":"2018-04-02T17:58:44+0000","dateStarted":"2018-04-06T09:13:30+0000","dateFinished":"2018-04-06T09:13:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1527"},{"text":"%pyspark\n\nfrom pyspark.sql.functions import split, to_date, to_timestamp, year, month, dayofmonth, concat, col, lit\n\n# Load CSV and Group Time by 10-minute split\ndef processCSVPath(spark, path):\n    df = spark.read.csv(path,\\\n                        header=\"true\",\\\n                        inferSchema=\"true\",\\\n                        dateFormat=\"dd/MM/yyyy\",\\\n                        timestampFormat=\"dd/MM/yyyy HH:mm\")\\\n                   .withColumn(\"Start Date\", to_timestamp(col(\"Start Date\"), \"dd/MM/yyyy HH:mm\"))\\\n                   .withColumn(\"End Date\", to_timestamp(col(\"End Date\"), \"dd/MM/yyyy HH:mm\"))\\\n                   .withColumn(\"Start TimeGroup\", to_timestamp(concat(col(\"Start Date\").substr(1,15),\\\n                                                                      lit(\"0\")),\\\n                                                               \"yyyy-MM-dd HH:mm\"))\\\n                   .withColumn(\"End TimeGroup\", to_timestamp(concat(col(\"End Date\").substr(1,15),\\\n                                                                    lit(\"0\")),\\\n                                                             \"yyyy-MM-dd HH:mm\"))\n    df.createOrReplaceTempView('raw_journey')\n    return spark.table('raw_journey')\n\ndef prepareBikepoints(spark, path):\n    spark.read.json(path).repartition(\"id\").createOrReplaceGlobalTempView('bikepoints')\n    return spark.sql('select * from global_temp.bikepoints')\n\n# Getting number of bikes DEPARTING a certain bikestation\ndef get_start_count(df):\n    return df.select(col(\"StartStation Id\").alias(\"Station Id\"),\\\n                     col(\"Start TimeGroup\").alias(\"Time\"))\\\n             .repartition(\"Station Id\")\\\n             .groupBy(\"Station Id\", \"Time\")\\\n             .count()\\\n             .select(\"Station Id\", \"Time\", col(\"count\").alias(\"Start Count\"))\n\n# Getting number of bikes ARRIVING a certain bikestation\ndef get_end_count(df):\n    return df.select(col(\"EndStation Id\").alias(\"Station Id\"),\\\n                     col(\"End TimeGroup\").alias(\"Time\"))\\\n             .repartition(\"Station Id\")\\\n             .groupBy(\"Station Id\", \"Time\")\\\n             .count()\\\n             .select(\"Station Id\", \"Time\", ccol(\"count\").alias(\"End Count\"))\n             \ndef merge_start_end(start_df, end_df):\n    return start_df.join(end_df, [\"Station Id\", \"Time\"], \"outer\")\\\n                   .fillna(0, [\"Start Count\", \"End Count\"])\n\ndef merge_journey_with_bikepoints_info(aggregated_journey_df, bikepoints_df):\n    return bikepoints_df.join(aggregated_journey_df, bikepoints_df[\"id\"] == aggregated_journey_df[\"Station Id\"])\\\n                        .select([\"id\",\\\n                                 \"district\",\\\n                                 \"location\",\\\n                                 \"lat\",\\\n                                 \"lng\",\\\n                                 year(\"Time\").alias(\"year\"),\\\n                                 month(\"Time\").alias(\"month\"),\\\n                                 dayofmonth(\"Time\").alias(\"day\"),\\\n                                 col(\"Time\").alias(\"time\"),\\\n                                 col(\"Start Count\").alias(\"start_count\"),\\\n                                 col(\"End Count\").alias(\"end_count\")])\n\n","user":"anonymous","dateUpdated":"2018-04-06T09:12:20+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1522698539878_1250716561","id":"20180402-194859_1823045805","dateCreated":"2018-04-02T19:48:59+0000","dateStarted":"2018-04-06T09:12:20+0000","dateFinished":"2018-04-06T09:12:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1528"},{"text":"%pyspark\n\nbikepoints_df = prepareBikepoints(spark, \"s3a://\" + s3_bucket + bikepoints_file)\nif debug:\n    bikepoints_df.printSchema()\n    bikepoints_df.orderBy(\"district\").show(10)\n","user":"anonymous","dateUpdated":"2018-04-06T09:12:22+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- district: string (nullable = true)\n |-- id: long (nullable = true)\n |-- lat: double (nullable = true)\n |-- lng: double (nullable = true)\n |-- location: string (nullable = true)\n\n+--------+---+---------+---------+-----------------+\n|district| id|      lat|      lng|         location|\n+--------+---+---------+---------+-----------------+\n| Aldgate|115|51.514233|-0.073537|    Braham Street|\n| Aldgate|779|51.514449|-0.077178|      Houndsditch|\n| Aldgate| 33|  51.5156|-0.070056|    Central House|\n| Aldgate|102|51.513406|-0.076793|     Jewry Street|\n| Aldgate|202|51.512363|-0.069542|     Leman Street|\n| Aldgate|263|51.514225| -0.08066|     St. Mary Axe|\n|   Angel|697|51.536392|-0.112721|Charlotte Terrace|\n|   Angel|365|51.530344|-0.100168|        City Road|\n|   Angel|254|51.530515|-0.106408|  Chadwell Street|\n|   Angel|326|51.532661|-0.099981|    Graham Street|\n+--------+---+---------+---------+-----------------+\nonly showing top 10 rows\n\n"}]},"apps":[],"jobName":"paragraph_1522697617689_-1918782460","id":"20180402-193337_380237381","dateCreated":"2018-04-02T19:33:37+0000","dateStarted":"2018-04-06T09:12:22+0000","dateFinished":"2018-04-06T09:12:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1529"},{"text":"%pyspark\n\n# This paragraph process CSV bikepoints data from S3 and put it into bikejourney_df\n# Should we have Kinesis Data stream in future, we should have another paragraph instead\n\nbikejourney_df = processCSVPath(spark, \"s3a://\" + s3_bucket + raw_path + files)\nif debug:\n    bikejourney_df = processCSVPath(spark, \"s3a://\" + s3_bucket + raw_path + files)\n    bikejourney_df.printSchema()\n    bikejourney_df.show(10)\n","user":"anonymous","dateUpdated":"2018-04-06T09:13:33+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- Rental Id: string (nullable = true)\n |-- Duration: string (nullable = true)\n |-- Bike Id: string (nullable = true)\n |-- End Date: timestamp (nullable = true)\n |-- EndStation Id: string (nullable = true)\n |-- EndStation Name: string (nullable = true)\n |-- Start Date: timestamp (nullable = true)\n |-- StartStation Id: string (nullable = true)\n |-- StartStation Name: string (nullable = true)\n |-- Start TimeGroup: timestamp (nullable = true)\n |-- End TimeGroup: timestamp (nullable = true)\n\n+---------+--------+-------+-------------------+-------------+--------------------+-------------------+---------------+--------------------+-------------------+-------------------+\n|Rental Id|Duration|Bike Id|           End Date|EndStation Id|     EndStation Name|         Start Date|StartStation Id|   StartStation Name|    Start TimeGroup|      End TimeGroup|\n+---------+--------+-------+-------------------+-------------+--------------------+-------------------+---------------+--------------------+-------------------+-------------------+\n| 14675128|    2589|   9207|2012-08-03 23:46:00|           49|Curzon Street, Ma...|2012-08-03 23:03:00|            301|Marylebone Lane, ...|2012-08-03 23:00:00|2012-08-03 23:40:00|\n| 14686550|    3774|   9207|2012-08-03 17:18:00|          265|Southwick Street,...|2012-08-03 16:15:00|            195|Milroy Walk, Sout...|2012-08-03 16:10:00|2012-08-03 17:10:00|\n| 14717402|   15568|   9207|2012-08-04 11:05:00|          359|Butler Place, Wes...|2012-08-04 06:46:00|             36|De Vere Gardens, ...|2012-08-04 06:40:00|2012-08-04 11:00:00|\n| 14735575|     600|   9207|2012-08-04 03:15:00|           36|De Vere Gardens, ...|2012-08-04 03:05:00|            345|Flood Street, Che...|2012-08-04 03:00:00|2012-08-04 03:10:00|\n| 14759007|     538|   9207|2012-08-05 17:04:00|           64|William IV Street...|2012-08-05 16:55:00|            338|Wellington Street...|2012-08-05 16:50:00|2012-08-05 17:00:00|\n| 14760534|    5410|   9207|2012-08-05 19:12:00|          259|Embankment (Horse...|2012-08-05 17:42:00|            160|Waterloo Place, S...|2012-08-05 17:40:00|2012-08-05 19:10:00|\n| 14761863|    2133|   9207|2012-08-05 21:54:00|          295|Swan Street, The ...|2012-08-05 21:19:00|            388|Southampton Stree...|2012-08-05 21:10:00|2012-08-05 21:50:00|\n| 14875532|    1152|   9207|2012-08-08 10:46:00|          424|Ebury Bridge, Pim...|2012-08-08 10:27:00|            360|Howick Place, Wes...|2012-08-08 10:20:00|2012-08-08 10:40:00|\n| 14878756|     366|   9207|2012-08-08 18:16:00|          356|South Kensington ...|2012-08-08 18:10:00|            220|Chelsea Green, Ch...|2012-08-08 18:10:00|2012-08-08 18:10:00|\n| 14879855|    2576|   9207|2012-08-08 14:13:00|          220|Chelsea Green, Ch...|2012-08-08 13:30:00|            345|Flood Street, Che...|2012-08-08 13:30:00|2012-08-08 14:10:00|\n+---------+--------+-------+-------------------+-------------+--------------------+-------------------+---------------+--------------------+-------------------+-------------------+\nonly showing top 10 rows\n\n"}]},"apps":[],"jobName":"paragraph_1522744784915_-1765093839","id":"20180403-083944_847018634","dateCreated":"2018-04-03T08:39:44+0000","dateStarted":"2018-04-06T09:13:33+0000","dateFinished":"2018-04-06T09:32:01+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1530"},{"text":"%pyspark\n\nfrom pyspark.sql.functions import year, month, dayofmonth, dayofyear\n\n# Get start count and end count dataframes, join them together\nstart_df = get_start_count(bikejourney_df)\nend_df = get_end_count(bikejourney_df)\nmerge_start_end(start_df, end_df).createOrReplaceTempView(\"aggregated_journey\")\n\naggregated_journey_df = spark.table(\"aggregated_journey\")\n\n# Join resulting dataframes with Bikepoint Information so we have those in datalake\njourney_with_bpinfo_df = merge_journey_with_bikepoints_info(aggregated_journey_df, bikepoints_df)\n\n","user":"anonymous","dateUpdated":"2018-04-04T13:34:42+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1522700187597_-1574888999","id":"20180402-201627_911393833","dateCreated":"2018-04-02T20:16:27+0000","dateStarted":"2018-04-04T13:34:42+0000","dateFinished":"2018-04-04T13:34:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1531"},{"text":"%pyspark\n\nstart_df.show(5)\nend_df.show(5)","user":"anonymous","dateUpdated":"2018-04-04T13:36:18+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------+-------------------+-----------+\n|Station Id|               Time|Start Count|\n+----------+-------------------+-----------+\n|       471|2018-01-25 06:20:00|          1|\n|       496|2018-01-24 00:00:00|          1|\n|       148|2018-01-24 08:00:00|          3|\n|       148|2018-01-25 08:40:00|          2|\n|       148|2018-01-26 17:00:00|          1|\n+----------+-------------------+-----------+\nonly showing top 5 rows\n\n+----------+-------------------+---------+\n|Station Id|               Time|End Count|\n+----------+-------------------+---------+\n|       148|2018-01-27 15:10:00|        1|\n|       148|2018-01-27 12:20:00|        1|\n|       148|2018-01-27 13:40:00|        1|\n|       471|2018-01-27 21:00:00|        1|\n|       471|2018-01-28 16:00:00|        1|\n+----------+-------------------+---------+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1522848885321_1135527162","id":"20180404-133445_750211701","dateCreated":"2018-04-04T13:34:45+0000","dateStarted":"2018-04-04T13:36:18+0000","dateFinished":"2018-04-04T13:36:34+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1532"},{"text":"%pyspark\n\njourney_with_bpinfo_df.coalesce(1)\\\n                      .write\\\n                      .option(\"mapreduce.fileoutputcommitter.algorithm.version\", \"2\")\\\n                      .partitionBy(\"year\", \"month\", \"day\")\\\n                      .parquet(\"s3a://\" + s3_bucket + processed_path + \"/journey/data\", mode=\"append\", compression=\"snappy\")\n","user":"anonymous","dateUpdated":"2018-04-05T19:50:43+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1522749740031_1162551366","id":"20180403-100220_1335164217","dateCreated":"2018-04-03T10:02:20+0000","dateStarted":"2018-04-03T18:47:50+0000","dateFinished":"2018-04-03T18:57:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1533"},{"text":"%pyspark\n\nprint bikepoints_df.agg({\"lat\": \"max\"}).collect()\nprint bikepoints_df.agg({\"lat\": \"min\"}).collect()\nprint bikepoints_df.agg({\"lng\": \"max\"}).collect()\nprint bikepoints_df.agg({\"lng\": \"min\"}).collect()","user":"anonymous","dateUpdated":"2018-04-05T15:57:40+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[Row(max(lat)=51.549369)]\n[Row(min(lat)=51.454752)]\n[Row(max(lng)=-0.002275)]\n[Row(min(lng)=-0.236769)]\n"}]},"apps":[],"jobName":"paragraph_1522772934519_-1876485182","id":"20180403-162854_707512292","dateCreated":"2018-04-03T16:28:54+0000","dateStarted":"2018-04-05T15:57:40+0000","dateFinished":"2018-04-05T15:57:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1534"},{"text":"%pyspark\n","user":"anonymous","dateUpdated":"2018-04-05T15:46:08+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1522943168002_-1912132700","id":"20180405-154608_662744965","dateCreated":"2018-04-05T15:46:08+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:1535"}],"name":"JourneyData","id":"2DCSG4ZCJ","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}